{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c012ad34-0e2d-4e51-bb7e-1efa0a17defb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    GlobalAveragePooling1D,\n",
    "    Dropout,\n",
    "    TextVectorization,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    LSTM,\n",
    "    MaxPooling1D,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1161982-b3ac-4aa0-9925-ebf88d733c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    X_train = pd.read_csv('./data/final/X_train.csv')\n",
    "    y_train = pd.read_csv('./data/final/y_train.csv')\n",
    "    X_val = pd.read_csv('./data/final/X_val.csv')\n",
    "    y_val = pd.read_csv('./data/final/y_val.csv')\n",
    "    \n",
    "    train_not_na_indices = (X_train['selftext'].notna())\n",
    "    val_not_na_indices = (X_val['selftext'].notna())\n",
    "    \n",
    "    X_train = X_train[train_not_na_indices]\n",
    "    X_val = X_val[val_not_na_indices]\n",
    "\n",
    "    y_train = y_train[train_not_na_indices]\n",
    "    y_val = y_val[val_not_na_indices]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b1d0eb-ede3-4cfa-8bb5-36ce71be9349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # try and except the TF tokenizer\n",
    "# try:\n",
    "#     tokenizer = tfds.features.text.Tokenizer()\n",
    "# except AttributeError:\n",
    "#     tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "# # create an instance of the Counter class\n",
    "# token_counts = Counter()\n",
    "\n",
    "# for example in data_tf_train:\n",
    "#     tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "#     token_counts.update(tokens)\n",
    "\n",
    "# print('Size of training vocabulary:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050b113-b9e2-45ef-99be-5ef0bc10be5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8c5d5d0-1905-40bb-9c60-933bfa063253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text)\n",
    "\n",
    "# X_train_v = X_train['selftext'].apply(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81343786-0e07-49d1-b072-bda52bea1982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 1: Tokenizer and pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ed4132-fc53-41ad-9890-408f44a57497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ae6d098-29d9-49fb-927d-b2058839f5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train[X_train['selftext'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3114e189-a8d7-4872-8410-ddbcacb7b228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8183 1084    6 ...    4 2698 1748]\n",
      " [   0  852  354 ... 9675 1039  102]\n",
      " [   0    0    0 ... 1255   13   12]\n",
      " ...\n",
      " [   0    0    0 ...  217 2062  578]\n",
      " [   0    0    0 ... 3057    3  925]\n",
      " [   0    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenizer setup\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(X_train['selftext'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(X_train['selftext'])\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 250\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e12f8968-6bfd-41a9-bb1c-61b6fbe7d721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78b6f8f0-937a-44ad-8c93-51b4091bbcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:19:59.112489: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:19:59.115413: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:19:59.116770: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-24 17:19:59.293108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:19:59.294273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:19:59.295745: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 16)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 250, 128)          74240     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 283,713\n",
      "Trainable params: 283,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:19:59.668487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:19:59.671003: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:19:59.673251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-24 17:19:59.923549: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:19:59.925960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:19:59.927699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-24 17:20:01.213988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:20:01.216614: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:20:01.218937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-24 17:20:01.476003: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:20:01.478457: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:20:01.480506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639/639 [==============================] - ETA: 0s - loss: 550785.1250 - mae: 87.0742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 17:35:34.321342: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:35:34.324987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:35:34.327636: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-24 17:35:34.730719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-24 17:35:34.733835: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-24 17:35:34.735405: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639/639 [==============================] - 1017s 2s/step - loss: 550785.1250 - mae: 87.0742 - val_loss: 2039985.7500 - val_mae: 106.4994\n",
      "Epoch 2/10\n",
      "639/639 [==============================] - 953s 1s/step - loss: 549058.6250 - mae: 94.9498 - val_loss: 2038535.5000 - val_mae: 113.4155\n",
      "Epoch 3/10\n",
      "639/639 [==============================] - 893s 1s/step - loss: 548087.3750 - mae: 101.3081 - val_loss: 2037464.1250 - val_mae: 119.9477\n",
      "Epoch 4/10\n",
      "639/639 [==============================] - 949s 1s/step - loss: 547450.2500 - mae: 107.6480 - val_loss: 2036765.2500 - val_mae: 125.1221\n",
      "Epoch 5/10\n",
      "639/639 [==============================] - 902s 1s/step - loss: 547190.8750 - mae: 112.0389 - val_loss: 2036262.6250 - val_mae: 129.5090\n",
      "Epoch 6/10\n",
      "639/639 [==============================] - 861s 1s/step - loss: 546764.3750 - mae: 116.0151 - val_loss: 2036002.8750 - val_mae: 132.0827\n",
      "Epoch 7/10\n",
      "639/639 [==============================] - 892s 1s/step - loss: 547236.9375 - mae: 112.0981 - val_loss: 2036952.2500 - val_mae: 123.6417\n",
      "Epoch 8/10\n",
      "639/639 [==============================] - 970s 2s/step - loss: 547354.6875 - mae: 112.1325 - val_loss: 2036412.5000 - val_mae: 128.1340\n",
      "Epoch 9/10\n",
      "639/639 [==============================] - 874s 1s/step - loss: 546702.0625 - mae: 115.5548 - val_loss: 2035947.8750 - val_mae: 132.6815\n",
      "Epoch 10/10\n",
      "639/639 [==============================] - 884s 1s/step - loss: 546842.4375 - mae: 119.0556 - val_loss: 2035680.3750 - val_mae: 135.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d54185810>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    layers.LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(1) # regression doesn't use an activation function\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "# Train the model\n",
    "model1.fit(vectorized_text, y_train, epochs=5, batch_size=2, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a84e4-3fb6-4698-8b5f-7ab05890942c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 2: TextVectorization Layer, Basic Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2589651-a306-4f87-98ab-3763c50faad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c6cfcec2-0b96-4ef9-bd67-9e23fe1d16fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectorization_layer(df, column, max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=output_sequence_length)\n",
    "\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorize_layer.adapt(df[column].values)\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "def get_embedding_layer(max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    return Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=output_sequence_length)\n",
    "\n",
    "def build_model_2(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "    \n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0cc5907-d4a1-43f8-bf6e-23330cc64f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "840546f0-25dd-4b52-9788-c38504d0e363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 842049.0000 - mean_absolute_error: 125.7566 - accuracy: 0.1303"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m build_model_2()\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history_2 \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mfit(vectorized_text, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history_2\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "model_2 = build_model_2()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_2 = model_2.fit(vectorized_text, y_train, epochs=5, batch_size=2, verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45477179-4208-496d-8235-95a86e7b4e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_full_model_2(max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "    \n",
    "    x = Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=output_sequence_length)(inputs)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8796b134-f5f0-481c-a646-cfdb09b3d44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12769/12771 [============================>.] - ETA: 0s - loss: 841407.3125 - mean_absolute_error: 125.6788 - accuracy: 0.1302WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 196s 15ms/step - loss: 841388.4375 - mean_absolute_error: 125.7218 - accuracy: 0.1302\n",
      "Epoch 2/5\n",
      "12768/12771 [============================>.] - ETA: 0s - loss: 820791.6875 - mean_absolute_error: 120.4560 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 201s 16ms/step - loss: 820631.0625 - mean_absolute_error: 120.4370 - accuracy: 0.1305\n",
      "Epoch 3/5\n",
      "12768/12771 [============================>.] - ETA: 0s - loss: 791915.6875 - mean_absolute_error: 119.8124 - accuracy: 0.1304WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 198s 15ms/step - loss: 791760.9375 - mean_absolute_error: 119.7966 - accuracy: 0.1305\n",
      "Epoch 4/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 754385.6250 - mean_absolute_error: 116.1661 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 199s 16ms/step - loss: 754356.0625 - mean_absolute_error: 116.1621 - accuracy: 0.1305\n",
      "Epoch 5/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 705847.8750 - mean_absolute_error: 113.2431 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 193s 15ms/step - loss: 705820.1875 - mean_absolute_error: 113.2395 - accuracy: 0.1305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841388.4375</td>\n",
       "      <td>125.721809</td>\n",
       "      <td>0.130222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820631.0625</td>\n",
       "      <td>120.436974</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>791760.9375</td>\n",
       "      <td>119.796585</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>754356.0625</td>\n",
       "      <td>116.162071</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>705820.1875</td>\n",
       "      <td>113.239540</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy\n",
       "0  841388.4375           125.721809  0.130222\n",
       "1  820631.0625           120.436974  0.130496\n",
       "2  791760.9375           119.796585  0.130496\n",
       "3  754356.0625           116.162071  0.130496\n",
       "4  705820.1875           113.239540  0.130496"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "\n",
    "model_2_full = build_full_model_2()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_2 = model_2_full.fit(vectorized_text, y_train, epochs=5, batch_size=2,verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_2.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18cdb1-ad29-4b7d-93e6-c4a3e5b02581",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 3: TextVectorization Layer, Convolutional NN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c481c-16f2-4b2a-983d-2cf8dc68d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4439e739-5de4-471e-83da-14e27bef9149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_3(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = layers.Conv1D(32, 4, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4897f3dc-1e1b-4855-a823-17cc270a5172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10059702-c6e7-4613-acea-a31ddd70ca4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10216/10216 [==============================] - 336s 33ms/step - loss: 547076.2500 - mean_absolute_error: 126.3061 - accuracy: 0.1295 - val_loss: 2035162.8750 - val_mean_absolute_error: 145.3031 - val_accuracy: 0.1333\n",
      "Epoch 2/5\n",
      "10216/10216 [==============================] - 322s 31ms/step - loss: 546597.9375 - mean_absolute_error: 130.1261 - accuracy: 0.1295 - val_loss: 2035472.1250 - val_mean_absolute_error: 141.1581 - val_accuracy: 0.1333\n",
      "Epoch 3/5\n",
      "10216/10216 [==============================] - 367s 36ms/step - loss: 546377.3750 - mean_absolute_error: 129.5938 - accuracy: 0.1295 - val_loss: 2035215.1250 - val_mean_absolute_error: 143.3732 - val_accuracy: 0.1333\n",
      "Epoch 4/5\n",
      "10216/10216 [==============================] - 455s 45ms/step - loss: 546211.4375 - mean_absolute_error: 130.3387 - accuracy: 0.1295 - val_loss: 2036054.5000 - val_mean_absolute_error: 142.9070 - val_accuracy: 0.1333\n",
      "Epoch 5/5\n",
      "10216/10216 [==============================] - 401s 39ms/step - loss: 545547.8125 - mean_absolute_error: 130.4532 - accuracy: 0.1295 - val_loss: 2037564.6250 - val_mean_absolute_error: 121.8255 - val_accuracy: 0.1333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_absolute_error</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>547076.2500</td>\n",
       "      <td>126.306091</td>\n",
       "      <td>0.129523</td>\n",
       "      <td>2035162.875</td>\n",
       "      <td>145.303070</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>546597.9375</td>\n",
       "      <td>130.126083</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2035472.125</td>\n",
       "      <td>141.158066</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>546377.3750</td>\n",
       "      <td>129.593842</td>\n",
       "      <td>0.129453</td>\n",
       "      <td>2035215.125</td>\n",
       "      <td>143.373245</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>546211.4375</td>\n",
       "      <td>130.338745</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2036054.500</td>\n",
       "      <td>142.906967</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545547.8125</td>\n",
       "      <td>130.453171</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2037564.625</td>\n",
       "      <td>121.825531</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy     val_loss  \\\n",
       "0  547076.2500           126.306091  0.129523  2035162.875   \n",
       "1  546597.9375           130.126083  0.129454  2035472.125   \n",
       "2  546377.3750           129.593842  0.129453  2035215.125   \n",
       "3  546211.4375           130.338745  0.129454  2036054.500   \n",
       "4  545547.8125           130.453171  0.129454  2037564.625   \n",
       "\n",
       "   val_mean_absolute_error  val_accuracy  \n",
       "0               145.303070      0.133294  \n",
       "1               141.158066      0.133294  \n",
       "2               143.373245      0.133294  \n",
       "3               142.906967      0.133294  \n",
       "4               121.825531      0.133294  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_model_3()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_3 = model_3.fit(vectorized_text, y_train, epochs=5, batch_size=2, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b2e7a8d-c6e2-4184-9b14-ce6f0721b7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_absolute_error</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>547076.2500</td>\n",
       "      <td>126.306091</td>\n",
       "      <td>0.129523</td>\n",
       "      <td>2035162.875</td>\n",
       "      <td>145.303070</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>546597.9375</td>\n",
       "      <td>130.126083</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2035472.125</td>\n",
       "      <td>141.158066</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>546377.3750</td>\n",
       "      <td>129.593842</td>\n",
       "      <td>0.129453</td>\n",
       "      <td>2035215.125</td>\n",
       "      <td>143.373245</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>546211.4375</td>\n",
       "      <td>130.338745</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2036054.500</td>\n",
       "      <td>142.906967</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545547.8125</td>\n",
       "      <td>130.453171</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>2037564.625</td>\n",
       "      <td>121.825531</td>\n",
       "      <td>0.133294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy     val_loss  \\\n",
       "0  547076.2500           126.306091  0.129523  2035162.875   \n",
       "1  546597.9375           130.126083  0.129454  2035472.125   \n",
       "2  546377.3750           129.593842  0.129453  2035215.125   \n",
       "3  546211.4375           130.338745  0.129454  2036054.500   \n",
       "4  545547.8125           130.453171  0.129454  2037564.625   \n",
       "\n",
       "   val_mean_absolute_error  val_accuracy  \n",
       "0               145.303070      0.133294  \n",
       "1               141.158066      0.133294  \n",
       "2               143.373245      0.133294  \n",
       "3               142.906967      0.133294  \n",
       "4               121.825531      0.133294  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history3.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00aabe6-8bf3-463b-af91-da2b7027d35e",
   "metadata": {},
   "source": [
    "# Model 4: TextVectorization Layer, LSTM RNN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c36742-1a81-4381-8864-05d8fa23e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd9bc3-beb1-4644-9a55-ad2bd71b2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_4(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = layers.LSTM(32, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6f2bf-a316-4dfc-a0b6-d4e7da596f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67983f5-c6c5-48ff-9801-11ad357805fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = build_model_4()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_4 = model_4.fit(vectorized_text, y_train, epochs=5, batch_size=2, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_4.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6b5c2-d31e-4cac-baf9-fad6d1b102e6",
   "metadata": {},
   "source": [
    "# Model 5: TextVectorization Layer, Bi-Directional LSTM RNN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8aee0607-4d43-4c66-8b6f-43cc057ad931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55470828-37ca-47b0-bd7c-783c3182887f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_5(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = Bidirectional(LSTM(32, activation='relu'))(x)\n",
    "    # x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a34bc69a-2aec-471f-8569-a1d0223ad600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "352203d4-3687-4d29-8953-59a376fa9e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   86/12771 [..............................] - ETA: 1:30:12 - loss: nan - mean_absolute_error: nan - accuracy: 0.1628                                     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_5 \u001b[38;5;241m=\u001b[39m build_model_5()\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history_5 \u001b[38;5;241m=\u001b[39m model_5\u001b[38;5;241m.\u001b[39mfit(vectorized_text, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history_5\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_5 = build_model_5()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_5 = model_5.fit(vectorized_text, y_train, epochs=5, batch_size=2, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_5.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d6e61-9d6b-4d64-b3b2-26f394ee5680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Using TextVectorization Layer 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74ca41a-309c-4d51-b82a-dff181ef71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "# Input shape:  (batch_size, input_length)\n",
    "# Output shape: (batch_size, input_length, output_dim)\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim = vocab_size,  # size of feature vocabulary\n",
    "    output_dim = 2,   # embedding dimension\n",
    "    input_length = max_sequence_length  # number of inputs\n",
    "    )\n",
    "\n",
    "first_review_embed_rep = embedding_layer(X_train_v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb185ebe-27bb-4a7c-86d5-621607994cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(vectorize_layer):\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = vocab_size,  # size of feature vocabulary\n",
    "        output_dim = 2,  # embedding dimension\n",
    "        input_length = max_sequence_length  # number of inputs\n",
    "        ))\n",
    "\n",
    "    # Average over the sequence dimension, so each review is represented by\n",
    "    # 1 vector of size embedding_dimension\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Alternatively, we could concatenate the embedding representations of\n",
    "    # all tokens in the movie review\n",
    "    #model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=8,\n",
    "      activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,\n",
    "      activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ae30b12-a671-4611-ad85-1e7565a364f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"text_vectorization_1\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization_1\" (type TextVectorization):\n   inputs=tf.Tensor(shape=(None, None), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(vectorize_layer)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Display the model layers.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m display(model\u001b[38;5;241m.\u001b[39mlayers)\n",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(vectorize_layer)\u001b[0m\n\u001b[1;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(vectorize_layer)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(\n\u001b[1;32m     10\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m vocab_size,  \u001b[38;5;66;03m# size of feature vocabulary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# embedding dimension\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     input_length \u001b[38;5;241m=\u001b[39m max_sequence_length  \u001b[38;5;66;03m# number of inputs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     ))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Average over the sequence dimension, so each review is represented by\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 1 vector of size embedding_dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/layers/preprocessing/text_vectorization.py:573\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be 1. Received: inputs.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"text_vectorization_1\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization_1\" (type TextVectorization):\n   inputs=tf.Tensor(shape=(None, None), dtype=string)"
     ]
    }
   ],
   "source": [
    "def get_vectorization_layer(df, column, max_features=10000, sequence_length=250, embedding_dim=16):\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorize_layer.adapt(df[column])\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "\n",
    "model = build_model(vectorize_layer)\n",
    "\n",
    "# Display the model layers.\n",
    "display(model.layers)\n",
    "display(model.summary())\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings = model.layers[1].get_weights()[0]\n",
    "print('|'*100)\n",
    "display(\"Embeddings layer - shape: \", embeddings.shape)\n",
    "print('|'*100)\n",
    "display(\"Embeddings layer - parameter matrix (before training): \", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75e35b-f237-46f2-9f6b-7bfec32289fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb09a9e-c2f4-49e0-a86a-405e2cd31273",
   "metadata": {},
   "source": [
    "# End of file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f819b9-4d42-4fbf-bf2d-c0ffd4809515",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* https://stackoverflow.com/questions/73878049/how-do-you-convert-the-pandas-dataframe-to-tensorflow-python-data-ops-dataset-op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9e68a-d697-4c41-b9c1-3dfaa36d33ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
