{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c012ad34-0e2d-4e51-bb7e-1efa0a17defb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    GlobalAveragePooling1D,\n",
    "    Dropout,\n",
    "    TextVectorization,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    LSTM,\n",
    "    MaxPooling1D,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1161982-b3ac-4aa0-9925-ebf88d733c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    X_train = pd.read_csv('./data/final/X_train.csv')\n",
    "    y_train = pd.read_csv('./data/final/y_train.csv')\n",
    "    X_val = pd.read_csv('./data/final/X_val.csv')\n",
    "    y_val = pd.read_csv('./data/final/y_val.csv')\n",
    "    \n",
    "    train_not_na_indices = (X_train['selftext'].notna())\n",
    "    val_not_na_indices = (X_val['selftext'].notna())\n",
    "    \n",
    "    X_train = X_train[train_not_na_indices]\n",
    "    X_val = X_val[val_not_na_indices]\n",
    "\n",
    "    y_train = y_train[train_not_na_indices]\n",
    "    y_val = y_val[val_not_na_indices]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b1d0eb-ede3-4cfa-8bb5-36ce71be9349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # try and except the TF tokenizer\n",
    "# try:\n",
    "#     tokenizer = tfds.features.text.Tokenizer()\n",
    "# except AttributeError:\n",
    "#     tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "# # create an instance of the Counter class\n",
    "# token_counts = Counter()\n",
    "\n",
    "# for example in data_tf_train:\n",
    "#     tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "#     token_counts.update(tokens)\n",
    "\n",
    "# print('Size of training vocabulary:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050b113-b9e2-45ef-99be-5ef0bc10be5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5d5d0-1905-40bb-9c60-933bfa063253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81343786-0e07-49d1-b072-bda52bea1982",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 1: Tokenizer and pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ed4132-fc53-41ad-9890-408f44a57497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae6d098-29d9-49fb-927d-b2058839f5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train[X_train['selftext'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3114e189-a8d7-4872-8410-ddbcacb7b228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[1;32m     35\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_sequence_length)\n\u001b[0;32m---> 37\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m vectorize_layer(text_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_data' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "def get_vectorization_layer(df, column, max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=output_sequence_length)\n",
    "\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorize_layer.adapt(df[column].values)\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "\n",
    "def vectorize_text(text):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text)\n",
    "\n",
    "\n",
    "\n",
    "X_train_v = X_train['selftext'].apply(vectorize_text)\n",
    "\n",
    "# Tokenizer setup\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(X_train['selftext'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(X_train['selftext'])\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 250\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "vectorized_text = vectorize_layer(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f8968-6bfd-41a9-bb1c-61b6fbe7d721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6f8f0-937a-44ad-8c93-51b4091bbcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    layers.LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(1) # regression doesn't use an activation function\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "# Train the model\n",
    "model1.fit(vectorized_text, y_train, epochs=5, batch_size=2, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a84e4-3fb6-4698-8b5f-7ab05890942c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 2: TextVectorization Layer, Basic Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2589651-a306-4f87-98ab-3763c50faad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6cfcec2-0b96-4ef9-bd67-9e23fe1d16fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectorization_layer(df, column, max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=output_sequence_length)\n",
    "\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorize_layer.adapt(df[column].values)\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "def get_embedding_layer(max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    return Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=output_sequence_length)\n",
    "\n",
    "def build_model_2(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "    \n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0cc5907-d4a1-43f8-bf6e-23330cc64f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "840546f0-25dd-4b52-9788-c38504d0e363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 841420.0625 - mean_absolute_error: 125.8751 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 194s 15ms/step - loss: 841420.0625 - mean_absolute_error: 125.8751 - accuracy: 0.1305\n",
      "Epoch 2/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 821035.6875 - mean_absolute_error: 120.8028 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 216s 17ms/step - loss: 821035.6875 - mean_absolute_error: 120.8028 - accuracy: 0.1305\n",
      "Epoch 3/5\n",
      "12768/12771 [============================>.] - ETA: 0s - loss: 792560.5000 - mean_absolute_error: 118.6174 - accuracy: 0.1304WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 215s 17ms/step - loss: 792405.6250 - mean_absolute_error: 118.6006 - accuracy: 0.1305\n",
      "Epoch 4/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 754938.1250 - mean_absolute_error: 115.3741 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 211s 17ms/step - loss: 754908.5625 - mean_absolute_error: 115.3704 - accuracy: 0.1305\n",
      "Epoch 5/5\n",
      "12768/12771 [============================>.] - ETA: 0s - loss: 705859.1250 - mean_absolute_error: 111.9285 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 221s 17ms/step - loss: 705723.0000 - mean_absolute_error: 111.9219 - accuracy: 0.1305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841420.0625</td>\n",
       "      <td>125.875099</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>821035.6875</td>\n",
       "      <td>120.802795</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>792405.6250</td>\n",
       "      <td>118.600639</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>754908.5625</td>\n",
       "      <td>115.370438</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>705723.0000</td>\n",
       "      <td>111.921883</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy\n",
       "0  841420.0625           125.875099  0.130496\n",
       "1  821035.6875           120.802795  0.130496\n",
       "2  792405.6250           118.600639  0.130496\n",
       "3  754908.5625           115.370438  0.130496\n",
       "4  705723.0000           111.921883  0.130496"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = build_model_2()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_2 = model_2.fit(vectorized_text, y_train, epochs=5, batch_size=2, verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45477179-4208-496d-8235-95a86e7b4e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_full_model_2(max_tokens=10000, output_sequence_length=250, embedding_dim=16):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "    \n",
    "    x = Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=output_sequence_length)(inputs)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8796b134-f5f0-481c-a646-cfdb09b3d44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 841941.1875 - mean_absolute_error: 125.8112 - accuracy: 0.1303WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 246s 19ms/step - loss: 841941.1875 - mean_absolute_error: 125.8112 - accuracy: 0.1303\n",
      "Epoch 2/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 823003.3125 - mean_absolute_error: 121.1050 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 196s 15ms/step - loss: 823003.3125 - mean_absolute_error: 121.1050 - accuracy: 0.1305\n",
      "Epoch 3/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 796670.6250 - mean_absolute_error: 119.5044 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 228s 18ms/step - loss: 796639.5000 - mean_absolute_error: 119.5015 - accuracy: 0.1305\n",
      "Epoch 4/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 763079.2500 - mean_absolute_error: 116.2360 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 208s 16ms/step - loss: 763079.2500 - mean_absolute_error: 116.2360 - accuracy: 0.1305\n",
      "Epoch 5/5\n",
      "12768/12771 [============================>.] - ETA: 0s - loss: 719045.6875 - mean_absolute_error: 113.6156 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 291s 23ms/step - loss: 718906.7500 - mean_absolute_error: 113.6064 - accuracy: 0.1305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841941.1875</td>\n",
       "      <td>125.811234</td>\n",
       "      <td>0.130261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>823003.3125</td>\n",
       "      <td>121.104996</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>796639.5000</td>\n",
       "      <td>119.501534</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>763079.2500</td>\n",
       "      <td>116.235962</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>718906.7500</td>\n",
       "      <td>113.606369</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy\n",
       "0  841941.1875           125.811234  0.130261\n",
       "1  823003.3125           121.104996  0.130496\n",
       "2  796639.5000           119.501534  0.130496\n",
       "3  763079.2500           116.235962  0.130496\n",
       "4  718906.7500           113.606369  0.130496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "\n",
    "model_2_full = build_full_model_2()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_2 = model_2_full.fit(vectorized_text, y_train, epochs=5, batch_size=2,verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_2.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18cdb1-ad29-4b7d-93e6-c4a3e5b02581",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 3: TextVectorization Layer, Convolutional NN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959c481c-16f2-4b2a-983d-2cf8dc68d8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4439e739-5de4-471e-83da-14e27bef9149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_3(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = layers.Conv1D(32, 4, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4897f3dc-1e1b-4855-a823-17cc270a5172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10059702-c6e7-4613-acea-a31ddd70ca4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12769/12771 [============================>.] - ETA: 0s - loss: 843449.5000 - mean_absolute_error: 128.2510 - accuracy: 0.1304WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 335s 26ms/step - loss: 843427.9375 - mean_absolute_error: 128.2946 - accuracy: 0.1304\n",
      "Epoch 2/5\n",
      "12769/12771 [============================>.] - ETA: 0s - loss: 841823.1875 - mean_absolute_error: 132.1136 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 417s 33ms/step - loss: 841724.6875 - mean_absolute_error: 132.1045 - accuracy: 0.1305\n",
      "Epoch 3/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 840341.8750 - mean_absolute_error: 132.1948 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 371s 29ms/step - loss: 840341.8750 - mean_absolute_error: 132.1948 - accuracy: 0.1305\n",
      "Epoch 4/5\n",
      "12770/12771 [============================>.] - ETA: 0s - loss: 839931.6250 - mean_absolute_error: 133.0815 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 417s 33ms/step - loss: 839900.0000 - mean_absolute_error: 133.0796 - accuracy: 0.1305\n",
      "Epoch 5/5\n",
      "12771/12771 [==============================] - ETA: 0s - loss: 839023.1875 - mean_absolute_error: 133.2374 - accuracy: 0.1305WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,accuracy\n",
      "12771/12771 [==============================] - 445s 35ms/step - loss: 839023.1875 - mean_absolute_error: 133.2374 - accuracy: 0.1305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>843427.9375</td>\n",
       "      <td>128.294571</td>\n",
       "      <td>0.130380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841724.6875</td>\n",
       "      <td>132.104477</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840341.8750</td>\n",
       "      <td>132.194839</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>839900.0000</td>\n",
       "      <td>133.079559</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>839023.1875</td>\n",
       "      <td>133.237381</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy\n",
       "0  843427.9375           128.294571  0.130380\n",
       "1  841724.6875           132.104477  0.130496\n",
       "2  840341.8750           132.194839  0.130496\n",
       "3  839900.0000           133.079559  0.130496\n",
       "4  839023.1875           133.237381  0.130496"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_model_3()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_3 = model_3.fit(vectorized_text, y_train, epochs=5, batch_size=2, verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b2e7a8d-c6e2-4184-9b14-ce6f0721b7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>843427.9375</td>\n",
       "      <td>128.294571</td>\n",
       "      <td>0.130380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841724.6875</td>\n",
       "      <td>132.104477</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840341.8750</td>\n",
       "      <td>132.194839</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>839900.0000</td>\n",
       "      <td>133.079559</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>839023.1875</td>\n",
       "      <td>133.237381</td>\n",
       "      <td>0.130496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_absolute_error  accuracy\n",
       "0  843427.9375           128.294571  0.130380\n",
       "1  841724.6875           132.104477  0.130496\n",
       "2  840341.8750           132.194839  0.130496\n",
       "3  839900.0000           133.079559  0.130496\n",
       "4  839023.1875           133.237381  0.130496"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history_3.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00aabe6-8bf3-463b-af91-da2b7027d35e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 4: TextVectorization Layer, LSTM RNN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c36742-1a81-4381-8864-05d8fa23e226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22fd9bc3-beb1-4644-9a55-ad2bd71b2744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_4(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = layers.LSTM(32, activation='relu')(x)\n",
    "    # x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1d6f2bf-a316-4dfc-a0b6-d4e7da596f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a67983f5-c6c5-48ff-9801-11ad357805fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 9416/12771 [=====================>........] - ETA: 12:01 - loss: nan - mean_absolute_error: nan - accuracy: 0.1370"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_4 \u001b[38;5;241m=\u001b[39m build_model_4()\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history_4 \u001b[38;5;241m=\u001b[39m model_4\u001b[38;5;241m.\u001b[39mfit(vectorized_text, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history_4\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_4 = build_model_4()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_4 = model_4.fit(vectorized_text, y_train, epochs=5, batch_size=2, verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_4.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6b5c2-d31e-4cac-baf9-fad6d1b102e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 5: TextVectorization Layer, Bi-Directional LSTM RNN Embedding Model, Two Hidden Dense Layers (64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8aee0607-4d43-4c66-8b6f-43cc057ad931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55470828-37ca-47b0-bd7c-783c3182887f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_5(output_sequence_length=250):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    inputs = Input(shape=(output_sequence_length,))\n",
    "\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    x = Bidirectional(LSTM(32, activation='relu'))(x)\n",
    "    # x = layers.MaxPooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a34bc69a-2aec-471f-8569-a1d0223ad600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = tf.constant(X_train['selftext'].values)\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "vectorized_text = vectorize_layer(text_data)\n",
    "embedding_layer = get_embedding_layer()\n",
    "embedded_text = embedding_layer(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "352203d4-3687-4d29-8953-59a376fa9e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   97/12771 [..............................] - ETA: 55:12 - loss: nan - mean_absolute_error: nan - accuracy: 0.1701                   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_5 \u001b[38;5;241m=\u001b[39m build_model_5()\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history_5 \u001b[38;5;241m=\u001b[39m model_5\u001b[38;5;241m.\u001b[39mfit(vectorized_text, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history_5\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3jp/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_5 = build_model_5()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "history_5 = model_5.fit(vectorized_text, y_train, epochs=5, batch_size=2, verbose=1, callbacks=[early_stopping])\n",
    "pd.DataFrame(history_5.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d6e61-9d6b-4d64-b3b2-26f394ee5680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Using TextVectorization Layer 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ca41a-309c-4d51-b82a-dff181ef71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "# Input shape:  (batch_size, input_length)\n",
    "# Output shape: (batch_size, input_length, output_dim)\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim = vocab_size,  # size of feature vocabulary\n",
    "    output_dim = 2,   # embedding dimension\n",
    "    input_length = max_sequence_length  # number of inputs\n",
    "    )\n",
    "\n",
    "first_review_embed_rep = embedding_layer(X_train_v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb185ebe-27bb-4a7c-86d5-621607994cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(vectorize_layer):\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = vocab_size,  # size of feature vocabulary\n",
    "        output_dim = 2,  # embedding dimension\n",
    "        input_length = max_sequence_length  # number of inputs\n",
    "        ))\n",
    "\n",
    "    # Average over the sequence dimension, so each review is represented by\n",
    "    # 1 vector of size embedding_dimension\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Alternatively, we could concatenate the embedding representations of\n",
    "    # all tokens in the movie review\n",
    "    #model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=8,\n",
    "      activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,\n",
    "      activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae30b12-a671-4611-ad85-1e7565a364f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectorization_layer(df, column, max_features=10000, sequence_length=250, embedding_dim=16):\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    df[column] = df[column].astype(str)\n",
    "    vectorize_layer.adapt(df[column])\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = get_vectorization_layer(X_train, 'selftext')\n",
    "\n",
    "model = build_model(vectorize_layer)\n",
    "\n",
    "# Display the model layers.\n",
    "display(model.layers)\n",
    "display(model.summary())\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings = model.layers[1].get_weights()[0]\n",
    "print('|'*100)\n",
    "display(\"Embeddings layer - shape: \", embeddings.shape)\n",
    "print('|'*100)\n",
    "display(\"Embeddings layer - parameter matrix (before training): \", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75e35b-f237-46f2-9f6b-7bfec32289fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb09a9e-c2f4-49e0-a86a-405e2cd31273",
   "metadata": {},
   "source": [
    "# End of file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f819b9-4d42-4fbf-bf2d-c0ffd4809515",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* https://stackoverflow.com/questions/73878049/how-do-you-convert-the-pandas-dataframe-to-tensorflow-python-data-ops-dataset-op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9e68a-d697-4c41-b9c1-3dfaa36d33ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
